{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ghailan and Gordan**\n",
    "\n",
    "Spring 2024\n",
    "\n",
    "CS 443: Bio-inspired Machine Learning\n",
    "\n",
    "# Project 0: TensorFlow and MNIST\n",
    "\n",
    "The goals of this project is to:\n",
    "- refresh your memory of TensorFlow\n",
    "- learn how to use new TensorFlow and NumPy functions that will be helpful later in the semester in simple examples\n",
    "- load and preprocess the MNIST dataset\n",
    "- build an simple artificial neural network using the TensorFlow low-level API and train it on MNIST.\n",
    "\n",
    "## AI Policy\n",
    "\n",
    "To improve the quality of your learning and out of fairness to your hardworking classmates, AI (e.g. ChatGPT, Copilot, etc.) should NOT be used in ANY way on this project and extensions. This includes both written analysis, plotting, and code. I will only grade your work, not the AI's. I will stop grading your project if I notice AI-generated content (in any capacity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 12:58:17.092061: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.show()\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: TensorFlow\n",
    "\n",
    "The goal of this task is to refresh your memory of TensorFlow and learn to perform tasks with TensorFlow that will come up later in the semester with simpler examples. Having the TensorFlow documentation open is a good idea:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf\n",
    "\n",
    "In most cases, each subtask requires only 1 line of code to accomplish. And no loops here! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Convert from NumPy to TensorFlow\n",
    "\n",
    "Convert the following Tensor to an ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is your array a NumPy ndarray? True\n",
      "Your array has shape (2, 3) and it should be (2, 3).\n",
      "Your array has dtype int32 and it should be int32.\n"
     ]
    }
   ],
   "source": [
    "tensor = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "tensor_np = tensor.numpy()\n",
    "\n",
    "print(f'Is your array a NumPy ndarray? {isinstance(tensor_np, np.ndarray)}')\n",
    "print(f'Your array has shape {tensor_np.shape} and it should be (2, 3).')\n",
    "print(f'Your array has dtype {tensor_np.dtype} and it should be int32.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Convert from TensorFlow to NumPy\n",
    "\n",
    "Convert the following ndarray to a TensorFlow Tensor with datatype `tf.int64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is your array a TF Tensor? True\n",
      "Your array has shape (2, 3) and it should be (2, 3).\n",
      "Your array has <dtype: 'int64'> and it should be <dtype: 'int64'>.\n"
     ]
    }
   ],
   "source": [
    "array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "array_tf = tf.convert_to_tensor(array, tf.int64)\n",
    "\n",
    "print(f'Is your array a TF Tensor? {isinstance(array_tf, tf.Tensor)}')\n",
    "print(f'Your array has shape {array_tf.shape} and it should be (2, 3).')\n",
    "print(f\"Your array has {array_tf.dtype} and it should be <dtype: 'int64'>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Convert data types\n",
    "\n",
    "Convert the following Tensor to a `tf.float32` datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your tensor has <dtype: 'float32'> and it should be <dtype: 'float32'>.\n"
     ]
    }
   ],
   "source": [
    "tensor = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "tensor_np = tf.cast(tensor,tf.float32)\n",
    "\n",
    "print(f\"Your tensor has {tensor_np.dtype} and it should be <dtype: 'float32'>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. One-hot coding\n",
    "\n",
    "- One-hot code the following int-coded class labels. Assume `C=5`.\n",
    "- Make the dtype `tf.float32`.\n",
    "- Here's a twist: make the \"off value\" `-1` instead of `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your tensor has <dtype: 'float32'> and it should be <dtype: 'float32'>.\n",
      "Your tensor is\n",
      "[[-1. -1.  1. -1. -1.]\n",
      " [-1. -1. -1.  1. -1.]\n",
      " [ 1. -1. -1. -1. -1.]\n",
      " [-1.  1. -1. -1. -1.]\n",
      " [-1.  1. -1. -1. -1.]]\n",
      "and it should be\n",
      "[[-1. -1.  1. -1. -1.]\n",
      " [-1. -1. -1.  1. -1.]\n",
      " [ 1. -1. -1. -1. -1.]\n",
      " [-1.  1. -1. -1. -1.]\n",
      " [-1.  1. -1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "labels = [2, 3, 0, 1, 1]\n",
    "labels = np.array(labels)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "hot = np.ones((labels.shape[0], 5))*-1\n",
    "hot[np.arange(labels.shape[0]), labels] = 1\n",
    "\n",
    "\n",
    "yh = tf.convert_to_tensor(hot, tf.float32)\n",
    "\n",
    "\n",
    "print(f\"Your tensor has {yh.dtype} and it should be <dtype: 'float32'>.\")\n",
    "print(f\"Your tensor is\\n{yh}\")\n",
    "print('and it should be')\n",
    "print('''[[-1. -1.  1. -1. -1.]\n",
    " [-1. -1. -1.  1. -1.]\n",
    " [ 1. -1. -1. -1. -1.]\n",
    " [-1.  1. -1. -1. -1.]\n",
    " [-1.  1. -1. -1. -1.]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e. (i) Find the index of the top-k items in each row of a Tensor\n",
    "\n",
    "*This is a multi-part task.*\n",
    "\n",
    "In this scenario, we want to find the index of the 2nd largest item in each row. While in this case we are interested in the `k=2` largest, your solution should in theory work for any `k`. This is like `argmax` within each row, but more configurable. We may want the index of the max (`k=1`), the 2nd largest (`k=2`), 3rd largest (`k=3`), and so forth. If there are ties for `k`-th place, the smallest column index is fine.\n",
    "\n",
    "Ultimately, we want the column indices to have a `tf.int32` data type.\n",
    "\n",
    "**Hint:** There is a hint in the title of this subtask..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_tensor is:\n",
      "[[7. 3. 1. 4.]\n",
      " [2. 2. 2. 3.]\n",
      " [7. 3. 4. 7.]\n",
      " [1. 6. 4. 0.]\n",
      " [1. 7. 2. 3.]\n",
      " [1. 8. 9. 2.]\n",
      " [9. 2. 7. 4.]]\n",
      "Column indices shape is (7,) and should be (7,).\n",
      "Column indices of the 2nd largest values in each row are:\n",
      "[3 0 3 2 3 1 2]\n",
      "They should be\n",
      "[3 0 3 2 3 1 2]\n",
      "Dtype is <dtype: 'int32'> as it should be <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "my_tensor = tf.cast(tf.random.uniform(maxval=10, shape=(7, 4), dtype=tf.int64), dtype=tf.float32)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "k=2 \n",
    "values, indices = tf.approx_top_k(my_tensor, k)\n",
    "\n",
    "col_indices = indices[:,k-1]\n",
    "\n",
    "print(f'my_tensor is:\\n{my_tensor}')\n",
    "print(f'Column indices shape is {col_indices.shape} and should be (7,).')\n",
    "print(f'Column indices of the 2nd largest values in each row are:\\n{col_indices}')\n",
    "print('They should be\\n[3 0 3 2 3 1 2]')\n",
    "print(f\"Dtype is {col_indices.dtype} as it should be <dtype: 'int32'>\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e. (ii) Generating sequences of values\n",
    "\n",
    "Generate a 1D Tensor of `tf.int32` values that go from 0 to the number of rows in `my_tensor` - 1, in steps of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row indices shape is (7,) and should be (7,).\n",
      "Your row indices are:\n",
      "[0 1 2 3 4 5 6]\n",
      "They should be\n",
      "[0 1 2 3 4 5 6]\n",
      "Dtype is <dtype: 'int32'> as it should be <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "array = np.arange(0, my_tensor.shape[0], 1)\n",
    "row_indices = tf.convert_to_tensor(array, tf.int32)\n",
    "print(f'Row indices shape is {row_indices.shape} and should be (7,).')\n",
    "print(f'Your row indices are:\\n{row_indices}')\n",
    "print('They should be\\n[0 1 2 3 4 5 6]')\n",
    "print(f\"Dtype is {row_indices.dtype} as it should be <dtype: 'int32'>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e. (iii) Stacking multiple Tensors together\n",
    "\n",
    "Now you have a list of row indices (`row_indices`) and a list of column indices (`col_indices`). Write code to stack them into a single Tensor where the first column holds the row indices and the second column holds the column indices. In other words, we are creating row-col index pairs in each row.\n",
    "\n",
    "Example:\n",
    "`rows: [0, 1, 2]`\n",
    "`cols: [9, 8, 7]`\n",
    "\n",
    "We want combined:\n",
    "```\n",
    "0, 9\n",
    "1, 8\n",
    "2, 7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows are: [0 1 2 3 4 5 6]\n",
      "Cols are: [3 0 3 2 3 1 2]\n",
      "Your combined Tensor is:\n",
      "[[0 3]\n",
      " [1 0]\n",
      " [2 3]\n",
      " [3 2]\n",
      " [4 3]\n",
      " [5 1]\n",
      " [6 2]] and it should be:\n",
      "[[0 5]\n",
      " [1 4]\n",
      " [2 3]\n",
      " [3 2]\n",
      " [4 1]\n",
      " [5 0]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE  !!!!!seems to have an error on the notebooks end\n",
    "rc_indices = tf.stack([row_indices, col_indices], 1)\n",
    "\n",
    "print(f'Rows are: {row_indices}')\n",
    "print(f'Cols are: {col_indices}')\n",
    "print(f'Your combined Tensor is:\\n{rc_indices} and it should be:')\n",
    "print('''[[0 5]\n",
    " [1 4]\n",
    " [2 3]\n",
    " [3 2]\n",
    " [4 1]\n",
    " [5 0]]''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e. (iv) Creating a mask in TensorFlow\n",
    "\n",
    "Let's say we want to create a Tensor of all 0s, except have some constant value at the locations of the 2nd largest items you found above in `my_tensor`. This Tensor should have the same shape as `my_tensor`. Let's make that constant value `99` here. Figure out how to use the TensorFlow function [scatter_nd](https://www.tensorflow.org/api_docs/python/tf/scatter_nd) to accomplish this.\n",
    "\n",
    "*Creating a Tensor that is the same shape as the original Tensor with all 0s except at preset locations is called a **mask**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 4)\n",
      "updated tensor is:\n",
      "[[ 0.  0.  0. 99.]\n",
      " [99.  0.  0.  0.]\n",
      " [ 0.  0.  0. 99.]\n",
      " [ 0.  0. 99.  0.]\n",
      " [ 0.  0.  0. 99.]\n",
      " [ 0. 99.  0.  0.]\n",
      " [ 0.  0. 99.  0.]] and should be:\n",
      "[[ 0.  0.  0. 99.]\n",
      " [99.  0.  0.  0.]\n",
      " [ 0.  0.  0. 99.]\n",
      " [ 0.  0. 99.  0.]\n",
      " [ 0.  0.  0. 99.]\n",
      " [ 0. 99.  0.  0.]\n",
      " [ 0.  0. 99.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(my_tensor.shape)\n",
    "mask_tensor = tf.scatter_nd(rc_indices, tf.ones(my_tensor.shape[0])*99, my_tensor.shape)\n",
    "\n",
    "print(f'updated tensor is:\\n{mask_tensor} and should be:')\n",
    "print('''[[ 0.  0.  0. 99.]\n",
    " [99.  0.  0.  0.]\n",
    " [ 0.  0.  0. 99.]\n",
    " [ 0.  0. 99.  0.]\n",
    " [ 0.  0.  0. 99.]\n",
    " [ 0. 99.  0.  0.]\n",
    " [ 0.  0. 99.  0.]]''')\n",
    "# print(f'Column indices of the 2nd largest values in each row are:\\n{tf.squeeze(col_indices)}')\n",
    "# print('They should be\\n[3 0 3 2 3 1 2]')\n",
    "# print(f\"Dtype is {col_indices.dtype} as it should be <dtype: 'int64'>\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e. (v) Updating a Tensor\n",
    "\n",
    "Recall that we cannot use assignment to update values in Tensors, but it is actually still possible to update them. Update the 2nd largest values in each row of `my_tensor`, replacing them with `99`. *This is just like the previous subtask, except the Tensor has the original values instead of `0`s in the non-`99` entries.*\n",
    "\n",
    "**Hint:** There is a helpful function that is very similar in name to `scatter_nd`. The name is very long..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated my_tensor is:\n",
      "[[ 7.  3.  1. 99.]\n",
      " [99.  2.  2.  3.]\n",
      " [ 7.  3.  4. 99.]\n",
      " [ 1.  6. 99.  0.]\n",
      " [ 1.  7.  2. 99.]\n",
      " [ 1. 99.  9.  2.]\n",
      " [ 9.  2. 99.  4.]] and it should be\n",
      "[[ 7.  3.  1. 99.]\n",
      " [99.  2.  2.  3.]\n",
      " [ 7.  3.  4. 99.]\n",
      " [ 1.  6. 99.  0.]\n",
      " [ 1.  7.  2. 99.]\n",
      " [ 1. 99.  9.  2.]\n",
      " [ 9.  2. 99.  4.]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "updated_tensor = tf.tensor_scatter_nd_update(my_tensor, rc_indices, tf.ones(my_tensor.shape[0])*99)\n",
    "\n",
    "print(f'The updated my_tensor is:\\n{updated_tensor} and it should be')\n",
    "print('''[[ 7.  3.  1. 99.]\n",
    " [99.  2.  2.  3.]\n",
    " [ 7.  3.  4. 99.]\n",
    " [ 1.  6. 99.  0.]\n",
    " [ 1.  7.  2. 99.]\n",
    " [ 1. 99.  9.  2.]\n",
    " [ 9.  2. 99.  4.]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1f. Generating random numbers\n",
    "\n",
    "Generate 7 uniformly random floats between -1 (inclusive) and +1 (exclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your random numbers are:\n",
      "[[-0.416 -0.587  0.071  0.123 -0.167  0.616 -0.014]]\n",
      "and should be\n",
      "[-0.416 -0.587  0.071  0.123 -0.167  0.616 -0.014]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)  # KEEP ME\n",
    "\n",
    "# YOUR CODE HERE\n",
    "rand_nums = tf.random.uniform((1,7), -1, 1)\n",
    "\n",
    "print(f'Your random numbers are:\\n{rand_nums}\\nand should be\\n[-0.416 -0.587  0.071  0.123 -0.167  0.616 -0.014]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1g. Exchanging rows and columns\n",
    "\n",
    "Exchange the rows and columns of the Tensor `a` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your tensor:\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]] and it should be:\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "a1 = tf.transpose(a)\n",
    "\n",
    "print(f'Your tensor:\\n{a1} and it should be:')\n",
    "print('''[[1 4]\n",
    " [2 5]\n",
    " [3 6]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1h. Indexing a tensor with non-contiguous indices\n",
    "\n",
    "Let's say we have the Tensor below:\n",
    "\n",
    "```\n",
    "[[ 0  1  2  3]\n",
    " [ 4  5  6  7]\n",
    " [ 8  9 10 11]\n",
    " [12 13 14 15]\n",
    " [16 17 18 19]\n",
    " [20 21 22 23]]\n",
    "```\n",
    "\n",
    "and we wanted to index into it so that we extract the 1st row, the last row, the 2nd row, the 2nd to last row, ... (in that order):\n",
    "\n",
    "```\n",
    "[[ 0,  1,  2,  3],\n",
    " [20, 21, 22, 23],\n",
    " [ 4,  5,  6,  7],\n",
    " [16, 17, 18, 19],\n",
    " [ 8,  9, 10, 11],\n",
    " [12, 13, 14, 15]]\n",
    "```\n",
    "\n",
    "In one line of code, use a function to accomplish this indexing operation (i.e. no square bracket syntax).\n",
    "\n",
    "**Hint:** This function was included in the TensorFlow tutorial from last semester, but we didn't actually use it for anything at the time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your tensor is:\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]\n",
      " [16 17 18 19]\n",
      " [20 21 22 23]]\n",
      "Your reordered tensor is:\n",
      "[[ 0  1  2  3]\n",
      " [20 21 22 23]\n",
      " [ 4  5  6  7]\n",
      " [16 17 18 19]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]] and should be\n",
      "[[ 0  1  2  3]\n",
      " [20 21 22 23]\n",
      " [ 4  5  6  7]\n",
      " [16 17 18 19]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "tensor = tf.reshape(tf.range(24), [6, 4])\n",
    "indices = [0, 5, 1, 4, 2, 3]  # List of inds of rows to extract from tensor (in that order). \n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "tensor_reordered = tf.gather(tensor, indices)\n",
    "print(f'Your tensor is:\\n{tensor}')\n",
    "print(f'Your reordered tensor is:\\n{tensor_reordered} and should be')\n",
    "print('''[[ 0  1  2  3]\n",
    " [20 21 22 23]\n",
    " [ 4  5  6  7]\n",
    " [16 17 18 19]\n",
    " [ 8  9 10 11]\n",
    " [12 13 14 15]]''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1i. Summing along an axis while preserving the number of dimensions\n",
    "\n",
    "The below operation `op` subtracts the sum of values within each row and normalizes so that the max within each column is 1. Your task is to define `row_sum`, which is the sum of all entries within each row. `row_sum` needs to retain a 2D shape for `op` not to crash (something like `(3, 1)`, not something that looks like `(3,)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of the operation is tensor is:\n",
      "[[0.538 0.48  0.417 0.375]\n",
      " [1.    1.    1.    1.   ]\n",
      " [0.115 0.16  0.208 0.25 ]] and it should be\n",
      "[[0.538 0.48  0.417 0.375]\n",
      " [1.    1.    1.    1.   ]\n",
      " [0.115 0.16  0.208 0.25 ]]\n"
     ]
    }
   ],
   "source": [
    "new_tensor = tf.constant([[1, 3, 5, 6], [7, 8, 9, 9], [3, 2, 1, 0]], dtype=tf.float32)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "row_sum = tf.reduce_sum(new_tensor, axis = 1, keepdims=True)\n",
    "\n",
    "\n",
    "op = (new_tensor - row_sum) / (tf.constant([-26, -25, -24, -24], dtype=tf.float32))\n",
    "print(f'The result of the operation is tensor is:\\n{op} and it should be')\n",
    "print('''[[0.538 0.48  0.417 0.375]\n",
    " [1.    1.    1.    1.   ]\n",
    " [0.115 0.16  0.208 0.25 ]]''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1j. Checking the device on which TensorFlow is executing your code\n",
    "\n",
    "Below, print out:\n",
    "\n",
    "1. the list of physical devices Tensorflow has access to on your computer that are of type `'CPU'`. For most of you, there should be one physical device entry that mentions \"CPU\".\n",
    "2. the list of physical devices Tensorflow has access to on your computer that are of type `'GPU'`. For most of you, this could be an empty list.\n",
    "\n",
    "An example printout is:\n",
    "\n",
    "```\n",
    "CPU devices:\n",
    "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
    "GPU devices:\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CPU devices:')\n",
    "\n",
    "print('GPU devices:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1k. Throwback to NumPy: logical indexing\n",
    "\n",
    "This subtask involves NumPy, not TensorFlow.\n",
    "\n",
    "In one line of code, use logical indexing to replace all positive values in the below 2D ndarray with 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "arr = np.random.uniform(-0.5, 0.5, size=(2, 10))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "print(f'After the replacement, your array looks like:\\n{arr}')\n",
    "print('and it should look like')\n",
    "print('''[[ 1.     1.     1.     1.    -0.076  1.    -0.062  1.     1.    -0.117]\n",
    " [ 1.     1.     1.     1.    -0.429 -0.413 -0.48   1.     1.     1.   ]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: MNIST Dataset\n",
    "\n",
    "This task focuses on accessing and preprocessing MNIST, a popular image dataset that we will use throughout the semester. MNIST has 60,000 training samples and 10,000 test samples. Each sample is a `28x28` image of a hand written digit. The digit in each sample image is the class label. The digit may be any in the range `0-9` ($C = 10$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import get_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Load and preprocess MNIST\n",
    "\n",
    "Implement and test `get_mnist(N_val, path)` and other helper functions in `mnist.py` to load in MNIST, preprocess the dataset, then create train/validation/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, x_val, y_val = get_mnist(1000)\n",
    "\n",
    "print('Preprocessed shapes:')\n",
    "print(f'{x_train.shape=} {x_test.shape=} {x_val.shape=}\\n{y_train.shape=} {y_test.shape=} {y_val.shape=}')\n",
    "print('--------------')\n",
    "print(f'Train min/max: {tf.reduce_min(x_train)}/{tf.reduce_max(x_train)}')\n",
    "print(f'Validation min/max: {tf.reduce_min(x_val)}/{tf.reduce_max(x_val)}')\n",
    "print(f'Test min/max: {tf.reduce_min(x_test)}/{tf.reduce_max(x_test)}')\n",
    "print(f'Training samples {x_train.dtype=}, Validation samples {x_val.dtype=}, Test samples {x_test.dtype=}')\n",
    "print(f'Training labels {y_train.dtype=}, Validation labels {y_val.dtype=}, Test labels {y_test.dtype=}')\n",
    "print(f'Labels present in training set: {tf.sort(tf.unique(y_train)[0])}')\n",
    "print(f'Labels present in validation set: {tf.sort(tf.unique(y_val)[0])}')\n",
    "print(f'Labels present  in test set: {tf.sort(tf.unique(y_test)[0])}')\n",
    "print(f'First 5 training labels: {y_train[:5]}')\n",
    "print(f'First 5 validation labels: {y_val[:5]}')\n",
    "print(f'First 5 test labels: {y_test[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see:\n",
    "\n",
    "    Preprocessed shapes:\n",
    "    x_train.shape=TensorShape([59000, 784]) x_test.shape=TensorShape([10000, 784]) x_val.shape=TensorShape([1000, 784])\n",
    "    y_train.shape=(59000,) y_test.shape=(10000,) y_val.shape=(1000,)\n",
    "    --------------\n",
    "    Train min/max: 0.0/1.0\n",
    "    Validation min/max: 0.0/1.0\n",
    "    Test min/max: 0.0/1.0\n",
    "    Training samples x_train.dtype=tf.float32, Validation samples x_val.dtype=tf.float32, Test samples x_test.dtype=tf.float32\n",
    "    Training labels y_train.dtype=dtype('int64'), Validation labels y_val.dtype=dtype('int64'), Test labels y_test.dtype=dtype('int64')\n",
    "    Labels present in training set: [0 1 2 3 4 5 6 7 8 9]\n",
    "    Labels present in validation set: [0 1 2 3 4 5 6 7 8 9]\n",
    "    Labels present  in test set: [0 1 2 3 4 5 6 7 8 9]\n",
    "    First 5 training labels: [9 3 3 0 0]\n",
    "    First 5 validation labels: [9 3 7 3 4]\n",
    "    First 5 test labels: [9 1 0 9 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Visualize MNIST samples\n",
    "\n",
    "Use `draw_grid_image()` in `viz.py` in the cell below to create a `10x10` grid of the first 100 MNIST validation sample images. Be sure to convert the 1st 100 samples from Tensor to NumPy ndarray format before passing them into `draw_grid_image`.\n",
    "\n",
    "This function places the image samples on a *single canvas* image (i.e. does not create subplots). Plotting one image instead an 2D array is more efficient (*you will be using this to visualize weights many times during training)*!\n",
    "\n",
    "If you selected the first `N_val` samples for your validation set, the first two rows of your image should consist of the following digits:\n",
    "\n",
    "```\n",
    "[[9 3 7 3 4 5 9 1 2 4]\n",
    " [9 7 8 8 4 1 2 8 4 2]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from viz import draw_grid_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement and train a single layer classification neural network\n",
    "\n",
    "To help refresh your memory on TensorFlow and artificial neural networks, you will implement a single layer softmax neural network in 100% TensorFlow low-level API (no NumPy). There is nothing different about this network than what you built last semester in the MLP project. This warm-up task will allow you to practice design skills that you have developed in CS 343 and apply them to a new situation. It is important that your code is 100% TensorFlow because you will train the network in Task 4 using a graphics processing unit (GPU) on the MNIST dataset that you prepared from Task 2. **You will be re-using this code in Project 1 so write efficient and clean code.**\n",
    "\n",
    "I encourage you to consult code and in-class tutorials from last semester to help jog your memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from single_layer_net import SingleLayerNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Implement `SingleLayerNetwork` class: basics\n",
    "\n",
    "In `single_layer_net`, `SingleLayerNetwork` is a parent class and implements general features that any single layer network should have. *There will be specific single layer neural networks below and in Project 1 that inherit from `SingleLayerNetwork`.*\n",
    "\n",
    "Start by implementing methods in the parent class that all the neural networks will share. Use the below test code to help you along.\n",
    "\n",
    "- Constructor\n",
    "- `get_wts(self)`\n",
    "- `get_b(self)`\n",
    "- `set_wts(self)`\n",
    "- `set_b(self)`\n",
    "- `one_hot(self, y, C, off_value=0)`\n",
    "- `accuracy(self, y_true, y_pred)`\n",
    "\n",
    "**Reminder:** All code in `single_layer_net.py` should be built using TensorFlow only (no Numpy). So there is no need to import numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: weight and bias initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "M, C = 3, 2\n",
    "nl = SingleLayerNetwork(M, C, wt_stdev=0.01)\n",
    "wts = nl.get_wts()\n",
    "b = nl.get_b()\n",
    "print(f\"Your wts are:\\n{wts}\\nand should be:\")\n",
    "print(\"\"\"<tf.Variable 'Variable:0' shape=(3, 2) dtype=float32, numpy=\n",
    "array([[ 0.015,  0.004],\n",
    "       [-0.004, -0.01 ],\n",
    "       [-0.012,  0.005]], dtype=float32)>\"\"\")\n",
    "print(f\"\\nYour bias is:\\n{b}\\nand should be\")\n",
    "print(\"\"\"<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([0.011, 0.002], dtype=float32)> \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 5\n",
    "y = tf.constant([0, 3, 2, 4])\n",
    "nl = SingleLayerNetwork(1, C)\n",
    "print(f'Your one-hot coding of {y} is:')\n",
    "tf.print(nl.one_hot(y, C))\n",
    "print('and should be:\\n[[1 0 0 0 0]\\n [0 0 0 1 0]\\n [0 0 1 0 0]\\n [0 0 0 0 1]]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = SingleLayerNetwork(1, 1)\n",
    "y1 = tf.constant([1, 2, 3, 1, 2, 3])\n",
    "y2 = tf.constant([1, 0, 3, 1, 0, 3])\n",
    "acc = nl.accuracy(y1, y2)\n",
    "print(f'Your accuracy is {acc:.2f} and should be 0.67')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Implement `SoftmaxNet` class\n",
    "\n",
    "Create a class in `single_layer_net.py` called `SoftmaxNet` that inherits from `SingleLayerNetwork`. Implement and test the following methods in your `SoftmaxNet` class:\n",
    "- `forward(self, x):` Do the forward pass with samples `x`. For the softmax network, this is Dense netIn followed by softmax netAct.\n",
    "- `loss(self, yh, net_act):` Computes cross-entropy loss with true classes `yh` (one-hot coded) and `net_act`.\n",
    "\n",
    "\n",
    "For cross-entropy loss, you can use the \"one-hot version\" of the equation (*since true class values $\\vec{yh}$ are one-hot coded coming in*):\n",
    "\n",
    "$$L = -\\frac{1}{B} \\sum_{i=1}^B \\sum_{c=1}^C yh_{ic} \\times log(\\text{netAct}_{ic})$$\n",
    "\n",
    "where $B$ is the mini-batch size and $C$ is the number of classes, like usual.\n",
    "\n",
    "#### Tips\n",
    "- If you are getting type compatibility problems, try casting to `tf.float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from single_layer_net import SoftmaxNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M, C = 4, 3, 5\n",
    "\n",
    "# test input samples\n",
    "tf.random.set_seed(0)\n",
    "x = tf.random.normal(shape=(N, M), dtype=tf.float32)\n",
    "\n",
    "sm_net = SoftmaxNet(M, C)\n",
    "sm_net.set_b(tf.constant([-0.062,  0.088, -0.148, 0.3, -0.002], dtype=tf.float32))\n",
    "sm_net.set_wts(tf.constant([[-0.062,  0.112,  0.127, 0.11, 0.12], \\\n",
    "    [ 0.143,  0.04 ,  0.063, -0.11, -0.12], \\\n",
    "    [-0.22 ,  0.189, -0.017, 0.13, 0.14]], dtype=tf.float32))\n",
    "test_net_act = sm_net.forward(x)\n",
    "\n",
    "print(f'Your net_act is:\\n{test_net_act}')\n",
    "print('and it should be:')\n",
    "print(\"\"\"[[0.172 0.209 0.186 0.248 0.185]\n",
    " [0.151 0.201 0.138 0.292 0.217]\n",
    " [0.181 0.237 0.17  0.237 0.174]\n",
    " [0.166 0.193 0.166 0.272 0.203]]\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M, C = 4, 3, 5\n",
    "\n",
    "# test input samples\n",
    "tf.random.set_seed(0)\n",
    "x = tf.random.normal(shape=(N, M), dtype=tf.float32)\n",
    "\n",
    "sm_net = SoftmaxNet(M, C)\n",
    "sm_net.set_b(tf.constant([-0.062,  0.088, -0.148, 0.3, -0.002], dtype=tf.float32))\n",
    "sm_net.set_wts(tf.constant([[-0.062,  0.112,  0.127, 0.11, 0.12], \\\n",
    "    [ 0.143,  0.04 ,  0.063, -0.11, -0.12], \\\n",
    "    [-0.22 ,  0.189, -0.017, 0.13, 0.14]], dtype=tf.float32))\n",
    "test_net_act = sm_net.forward(x)\n",
    "test_yh = sm_net.one_hot(y, C)\n",
    "test_loss = sm_net.loss(test_yh, test_net_act)\n",
    "\n",
    "print(f'Your loss is {test_loss:.4f} and it should be 1.5895')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Single layer network shared functionality (`predict`)\n",
    "\n",
    "Now that you've implemented the specific methods for the softmax net, implement the `predict(self, x, net_act=None)` in the `SingleLayerNetwork` class, which predicts the class of each sample in `x`.\n",
    "\n",
    "To predict the class of sample `i` $c_i^*$, you can use the softmax activations in the output layer:\n",
    "$$c_i^* = argmax_{c}(\\text{netAct}_{ic})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M, C = 4, 3, 3\n",
    "\n",
    "# test input samples\n",
    "tf.random.set_seed(5)\n",
    "x = tf.random.normal(shape=(N, M), dtype=tf.float32)\n",
    "\n",
    "sm_net = SoftmaxNet(M, C)\n",
    "sm_net.set_b(tf.constant([-0.062,  0.088, -0.148], dtype=tf.float32))\n",
    "sm_net.set_wts(tf.constant([[-0.062,  0.112,  0.127], \\\n",
    "    [ 0.143,  0.04 ,  0.063], \\\n",
    "    [-0.22 ,  0.189, -0.017]], dtype=tf.float32))\n",
    "test_preds = sm_net.predict(x)\n",
    "\n",
    "tf.print(f'Your test predicted classes are {test_preds} and they should be [1 0 1 1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Single layer network shared functionality (`extract_at_indices` and `fit`)\n",
    "\n",
    "Next implement:\n",
    "- `extract_at_indices(self, x, indices)`: small helper method that that returns the samples or labels (`x`) at the indices `indices`. Useful for forming mini-batches during training.\n",
    "- `fit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `extract_at_indices`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SoftmaxNet(1, 1)\n",
    "\n",
    "# Test 1\n",
    "test_samples = tf.reshape(tf.range(100, dtype=tf.float32), [25, 4])\n",
    "test_1_inds = tf.constant([0, 10, 2])\n",
    "test_1_vals = sd.extract_at_indices(test_samples, test_1_inds)\n",
    "print(f'Test 1:\\n-------\\n {test_1_vals=}\\n and should be')\n",
    "print(\"\"\" test_1_vals=<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
    "array([[ 0.,  1.,  2.,  3.],\n",
    "       [40., 41., 42., 43.],\n",
    "       [ 8.,  9., 10., 11.]], dtype=float32)>\"\"\")\n",
    "\n",
    "# Test 2\n",
    "tf.random.set_seed(0)\n",
    "test_labels = tf.random.shuffle(tf.range(10))\n",
    "test_2_inds = tf.constant([1, 2, 3, 4])\n",
    "test_2_vals = sd.extract_at_indices(test_labels, test_2_inds)\n",
    "print(f'Test 2:\\n-------\\n {test_2_vals=}\\n and should be')\n",
    "print(\"\"\" test_2_vals=<tf.Tensor: shape=(4,), dtype=int32, numpy=array([9, 1, 2, 7], dtype=int32)>\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `fit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code in the cell below to train a `SoftmaxNet` on MNIST using the default hyperparameters (1 epoch).\n",
    "\n",
    "After 1 epoch of training, you should get a training and validation losses of ~`2-3` and validation accuracy of ~`14-15%`\n",
    "\n",
    "Suggested information to include in training progress print outs:\n",
    "\n",
    "```\n",
    "Epoch 0/0, Training loss 2.56, Val loss 2.47, Val acc 14.10 \n",
    "Finished training after 1 epochs!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)  # keep me\n",
    "\n",
    "# TODO: Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e. Train `SoftmaxNet` on MNIST\n",
    "\n",
    "In the cell below, train your `SoftmaxNet` with default hyperparameters and vary only the number of training epochs and `val_every`. Your goal is to train the network until the validation accuracy peaks. Do this by monitoring the validation accuracy while training. You should:\n",
    "1. Report the approximate number of training epochs needed so that the validation accuracy appears to reach its maximum. *You don't need to find this precisely, just approximately given the resolution of your print outs.*\n",
    "2. Create a high quality, well-labeled plot showing the training loss over every epoch of training.\n",
    "3. Create a high quality, well-labeled plot showing the validation loss only on epochs where you checked it. *You can combine with the previous plot or keep separate.*\n",
    "4. Report the the MNIST test set accuracy.\n",
    "\n",
    "**Tips:**\n",
    "- I would suggest NOT checking the validation accuracy every epoch! This will slow down training quite a bit. Find a frequency/interval that is more reasonable, but still gives you clear information about approximately when the validation accuracy peaks. \n",
    "- Using NumPy in plotting code below is totally fine and expected.\n",
    "- If training does not finish in 5 minutes, stop training and move onto Task 4 to get setup with a cloud GPU. Re-run the training with the GPU once that is configured and training should take less than 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)  # keep me\n",
    "\n",
    "# TODO: Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Training on GPUs\n",
    "\n",
    "The training workloads in CS443 will be sufficiently computationally demanding that running them on your laptop CPU will not be practical or a good use of your time. While you should develop and debug your project code locally on your computer like last semester, you will want to run your large \"production\" training jobs on graphics processing units (GPUs). While every computer has a GPU, many are not good for machine learning workflows or do not support training with TensorFlow. A good rule of thumb for what a \"large\" training run is ~5 minutes or more on your computer.\n",
    "\n",
    "You have several options for working with GPUs in this class:\n",
    "\n",
    "**Option 1** (recommended): You run your code on [Paperspace](https://www.paperspace.com). This involves uploading your project folder and all data/python/notebook files to the Paperspace web interface then simply opening the project notebook in the web interface. It costs money (we have a budget kindly offered by DavisAI), so we need to be mindful about usage. In my testing, this is slightly faster than Option 2. If you decide to go this route, your team needs to message me so that I can setup a Paperspace environment for your team.\n",
    "\n",
    "**Option 2**: You run your code on [Google Colab](https://colab.research.google.com). This involves uploading your project folder and all data/python/notebook files to a folder on Google Drive, opening the notebook from within Google Drive (to open it in Google Colab), selecting that you want to run your code on the GPU from a dropdown menu, and run a code snippet to connect your code with Google Drive. You are able to train on the GPU for free for up to ~12 hours at a time (should be enough time for CS443 work).\n",
    "\n",
    "**Option 3**: You happen to have a computer with powerful NVIDIA GPU or an Apple Mac with M1 Pro/Max, M2 Pro/Max, M3 Pro/Max chips (*regular M1, M2, and M3 may not be sufficient*). If you follow the instructions on the CS443 website to setup GPU support in TensorFlow, your computers should be able to handle the projects without using a cloud GPU.\n",
    "\n",
    "Check out the CS443 website for more detailed instructions. Once you decide, move onto the next subtask.\n",
    "\n",
    "*Note: you can change your mind at any time if an option above is not working well for you!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4a. Experiment: Effect of batch size on validation accuracy\n",
    "\n",
    "In the cell below, run the command from Task 1 that prints out GPU devices that TensorFlow is configured to run compute jobs on. You should see something like:\n",
    "\n",
    "```\n",
    "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, copy-paste your network training code from Task 3e. Run an experiment where you change the mini-batch size to:\n",
    "- 2048 (the default)\n",
    "- 8192\n",
    "- 50000 (i.e. batch gradient descent)\n",
    "\n",
    "Use the number of epochs that you settled on from Task 3e that yielded peak validation accuracy. Save/print off:\n",
    "\n",
    "1. The final validation accuracy.\n",
    "2. The runtime of `fit` (e.g. by using `time.time()`).\n",
    "\n",
    "**Note:** Don't forget to re-create/wipe your network clean in between training runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4b. Questions\n",
    "\n",
    "**Question 1:** For the 3 mini-batch sizes that you tested, how does the final validation accuracy change based on the mini-batch choices? Be specific.\n",
    "\n",
    "**Question 2:** For the 3 mini-batch sizes that you tested, how does the runtime differ? Be specific and relate your answer to you found in Question 1.\n",
    "\n",
    "**Question 3:** Why do you think you observed the trend that you did? The result should be fairly striking.  \n",
    "\n",
    "*It turns out that this result is fairly common in many different training scenarios. Keep this in mind going forward and when you think about selecting mini-batch sizes.*\n",
    "\n",
    "**Reminder:** You should not use AI in projects, including in your answers to questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:**\n",
    "\n",
    "**Answer 2:**\n",
    "\n",
    "**Answer 3:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "### a. AI Policy\n",
    "\n",
    "The goal of extensions is to learn and create something new beyond the base project that excites you. This is a reminder that projects (and extensions) with AI generated content will not be graded. This includes code and written text.\n",
    "\n",
    "### b. Guidelines\n",
    "\n",
    "To receive credit for any extension, you must:\n",
    "1. **You must describe what you did and what you found in detail**.\n",
    "2. Include (*labeled!*) plots and/or numbers to present your results.\n",
    "3. Write up your extensions below or in a separate notebook.\n",
    "4. Give kudos to all sources, including anyone that you consulted.\n",
    "\n",
    "### c. Suggestions\n",
    "\n",
    "**Rule of thumb: one deep, thorough extension is worth more than several quick, shallow extensions!**\n",
    "\n",
    "The ideas below are **suggested** extensions â€” feel free to go in another direction related to this project that is not listed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explore and learn more about TensorFlow\n",
    "\n",
    "Peruse the [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf) and document what you explore and learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tune your softmax network\n",
    "\n",
    "Do a hyperparameter search and see if you improve upon the current test accuracy you obtained in the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add early stopping support to your training pipeline\n",
    "\n",
    "Early stopping allows you to end training before the preset number of training epochs if the validation loss does not improve over a certain number of epochs. Integrate optional support for this in your `fit` method and demonstrate how it may be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Add learning rate decay support to your training pipeline\n",
    "\n",
    "Learning rate decay decreases the learning rate over the course of training to more closely hone in on the final global/local minimum that your neural network converges on. A simple, common, and effective method is to decrease the learning rate by some percentage every X epochs. Integrate optional support for this in your `fit` method and demonstrate how it may be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Other datasets\n",
    "\n",
    "Apply your softmax network to other datasets. Fashion MNIST and eMNIST are good options."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs343",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
