{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAMES HERE**\n",
    "\n",
    "Fall 2023\n",
    "\n",
    "CS 343: Neural Networks\n",
    "\n",
    "Project 2: Multi-layer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for obtaining the STL-dataset\n",
    "import load_stl10_dataset\n",
    "\n",
    "# for preprocessing dataset\n",
    "import preprocess_data\n",
    "\n",
    "# Set the color style so that Professor Layton can see your plots\n",
    "plt.show()\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "# Make the font size larger\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Turn off scientific notation when printing\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def plot_cross_entropy_loss(loss_history):\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Training mini-batch')\n",
    "    plt.ylabel('Loss (cross-entropy)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data\n",
    "\n",
    "### a. STL-10\n",
    "\n",
    "Run your function to load in the preprocessed STL-10 data in the following split:\n",
    "\n",
    "- 3000 training samples\n",
    "- 750 test samples\n",
    "- 1000 validation samples\n",
    "- 250 samples for development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl_labels = np.load(\"numpy/labels.npy\")\n",
    "stl_imgs = np.load(\"numpy/images.npy\")\n",
    "stl_imgs_pp, stl_labels_pp = preprocess_data.preprocess_stl(stl_imgs, stl_labels)\n",
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.create_splits(stl_imgs_pp, stl_labels_pp, 3000, 750, 1000, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Circle in a square\n",
    "\n",
    "The circle in a square (CIS) dataset is a simple binary classification dataset that is useful for debugging and visualizing what your MLP is learning. Points with (x, y) coordinates inside a circle have class value of 1, points with coordinates outside the circle have class value of 0. Training on the CIS dataset allows us to answer the question: can the MLP learn to discriminate whether a test point falls inside or outside the circle?\n",
    "\n",
    "#### Todo\n",
    "\n",
    "- Download the CIS dataset then run the cell below to load in the CIS train (`cis_train.dat`) and test (`cis_test.dat`) sets as numpy arrays.\n",
    "- Below, make a scatterplot showing the test set data. Color-code samples based on their class. If everything goes well, you should see a...solid, filled in circle inside unit square :)\n",
    "    - Make the aspect ratio of your x, y plotting axes equal, otherwiwse you might see an ellipse!\n",
    "\n",
    "In case you're curious about the data format:\n",
    "- Like usual, each row is a different sample.\n",
    "- The x-coordinate feature is the 1st column\n",
    "- The y-coordinate feature is the 2nd column\n",
    "- The class label (0 or 1) is in the third column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIS Train data shape:  (80, 2)\n",
      "CIS Train labels shape:  (80,)\n",
      "CIS Validation data shape:  (20, 2)\n",
      "CIS Validation labels shape:  (20,)\n",
      "CIS Test data shape:  (10000, 2)\n",
      "CIS Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "val_size = 20\n",
    "\n",
    "cis_train_path = os.path.join('data','cis_train.dat')\n",
    "cis_test_path = os.path.join('data','cis_test.dat')\n",
    "\n",
    "cis_train_all = np.loadtxt(cis_train_path, delimiter='\\t')\n",
    "\n",
    "# shuffle the data\n",
    "s_inds = np.arange(len(cis_train_all))\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(s_inds)\n",
    "\n",
    "cis_train_all = cis_train_all[s_inds]\n",
    "\n",
    "cis_train_x = cis_train_all[:, :2]\n",
    "cis_train_y = cis_train_all[:, 2].astype(int)\n",
    "\n",
    "cis_val_x = cis_train_x[:val_size]\n",
    "cis_train_x = cis_train_x[val_size:]\n",
    "cis_val_y = cis_train_y[:val_size]\n",
    "cis_train_y = cis_train_y[val_size:]\n",
    "\n",
    "cis_test_all = np.loadtxt(cis_test_path, delimiter='\\t')\n",
    "cis_test_x = cis_test_all[:, :2]\n",
    "cis_test_y = cis_test_all[:, 2].astype(int)\n",
    "\n",
    "print ('CIS Train data shape: ', cis_train_x.shape)\n",
    "print ('CIS Train labels shape: ', cis_train_y.shape)\n",
    "print ('CIS Validation data shape: ', cis_val_x.shape)\n",
    "print ('CIS Validation labels shape: ', cis_val_y.shape)\n",
    "print ('CIS Test data shape: ', cis_test_x.shape)\n",
    "print ('CIS Test labels shape: ', cis_test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(cis_test_x[:,0], cis_test_x[:,1], c=cis_test_y)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement Multilayer Perceptron (MLP) with softmax activation and cross-entropy loss\n",
    "\n",
    "Now that we've tested the softmax activation function and cross-entropy loss functions in a single-layer net, let's implement the MLP version.\n",
    "\n",
    "Much of your work on the single layer net will carry over, so go ahead and copy-paste and modify as needed!\n",
    "\n",
    "#### The structure of our MLP\n",
    "\n",
    "```\n",
    "Input layer (X units) ->\n",
    "Hidden layer (Y units) with Rectified Linear activation (ReLu) ->\n",
    "Output layer (Z units) with softmax activation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Implement the following functions in `mlp.py`\n",
    "\n",
    "- `initialize_wts`\n",
    "- `accuracy`\n",
    "- `one_hot`\n",
    "- `predict`\n",
    "- `forward`\n",
    "- `backward`\n",
    "- `fit`: see note below.\n",
    "\n",
    "#### Updates to `fit`\n",
    "\n",
    "Now that you have built and fit several neural networks, let's make a helpful update to the training process: in addition to recording the training loss and accuracy, record the accuracy on the **validation set** after every training epoch. Every `print_every` epochs print out validation accuracy to monitor performance on the validation set. See the `fit` docstring for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Test key functions with randomly generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy net for debugging\n",
    "num_inputs = 3\n",
    "num_features = 6\n",
    "num_hidden_units = 7\n",
    "num_classes = 5\n",
    "\n",
    "net = MLP(num_features, num_hidden_units, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test input shape: (3, 6)\n",
      "Test class vector shape: (3,)\n"
     ]
    }
   ],
   "source": [
    "# Generate random data and classes\n",
    "np.random.seed(0)\n",
    "test_x = np.random.normal(loc=0, scale=100, size=(num_inputs, num_features))\n",
    "test_y = np.random.uniform(low=0, high=num_classes-1, size=(num_inputs,))\n",
    "test_y = test_y.astype(int)\n",
    "print(f'Test input shape: {test_x.shape}')\n",
    "print(f'Test class vector shape: {test_y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `initialize_wts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y wt shape is (6, 7) and should be (6, 7)\n",
      "y bias shape is (7,) and should be (7,)\n",
      "z wt shape is (7, 5) and should be (7, 5)\n",
      "z bias shape is (5,) and should be (5,)\n",
      "1st few y wts are\n",
      "[ 0.018 -0.002  0.004  0.007  0.015  0.002]\n",
      "and should be\n",
      "[ 0.018 -0.002  0.004  0.007  0.015  0.002]\n",
      "y bias is\n",
      "[ 0.016 -0.006 -0.005 -0.011  0.009 -0.023  0.017]\n",
      "and should be\n",
      "[ 0.016 -0.006 -0.005 -0.011  0.009 -0.023  0.017]\n",
      "1st few z wts are\n",
      "[-0.004 -0.008  0.006 -0.006 -0.009 -0.002 -0.003]\n",
      "and should be\n",
      "[-0.004 -0.008  0.006 -0.006 -0.009 -0.002 -0.003]\n",
      "z bias is\n",
      "[ 0.018  0.004  0.001 -0.019 -0.003]\n",
      "and should be\n",
      "[ 0.018  0.004  0.001 -0.019 -0.003]\n"
     ]
    }
   ],
   "source": [
    "net.initialize_wts(M=num_features, H=num_hidden_units, C=num_classes, std=0.01)\n",
    "print(f'y wt shape is {net.y_wts.shape} and should be (6, 7)')\n",
    "print(f'y bias shape is {net.y_b.shape} and should be (7,)')\n",
    "print(f'z wt shape is {net.z_wts.shape} and should be (7, 5)')\n",
    "print(f'z bias shape is {net.z_b.shape} and should be (5,)')\n",
    "\n",
    "print(f'1st few y wts are\\n{net.y_wts[:,0]}\\nand should be\\n[ 0.018 -0.002  0.004  0.007  0.015  0.002]')\n",
    "print(f'y bias is\\n{net.y_b}\\nand should be\\n[ 0.016 -0.006 -0.005 -0.011  0.009 -0.023  0.017]')\n",
    "print(f'1st few z wts are\\n{net.z_wts[:,0]}\\nand should be\\n[-0.004 -0.008  0.006 -0.006 -0.009 -0.002 -0.003]')\n",
    "print(f'z bias is\\n{net.z_b}\\nand should be\\n[ 0.018  0.004  0.001 -0.019 -0.003]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `predict` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes are [1 1 1] and should be [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "test_y_pred = net.predict(test_x)\n",
    "print(f'Predicted classes are {test_y_pred} and should be [1 1 1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `forward` method focusing on`ReLU`(net act of hidden layer `y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your y activation is\n",
      "[[7.676 4.464 0.799 9.97  0.    0.    0.   ]\n",
      " [2.386 2.711 2.174 2.541 0.366 0.    0.   ]\n",
      " [4.013 2.665 1.19  3.023 0.    0.    0.   ]]\n",
      "The correct y activation (ReLU) is\n",
      "[[7.676 4.464 0.799 9.97  0.    0.    0.   ]\n",
      " [2.386 2.711 2.174 2.541 0.366 0.    0.   ]\n",
      " [4.013 2.665 1.19  3.023 0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "_,y_net_act_test,_,_,_ = net.forward(test_x, test_y)\n",
    "\n",
    "correct_y_act = np.array([[7.676, 4.464, 0.799, 9.97 , 0.   , 0.   , 0.   ],\n",
    "       [2.386, 2.711, 2.174, 2.541, 0.366, 0.   , 0.   ],\n",
    "       [4.013, 2.665, 1.19 , 3.023, 0.   , 0.   , 0.   ]])\n",
    "\n",
    "print(f'Your y activation is\\n{y_net_act_test}')\n",
    "print(f'The correct y activation (ReLU) is\\n{correct_y_act}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your z activation (class probabilities) is\n",
      "[[0.193 0.223 0.194 0.21  0.18 ]\n",
      " [0.2   0.219 0.194 0.194 0.193]\n",
      " [0.2   0.216 0.191 0.204 0.189]]\n",
      "The correct z activation (class probabilities) is\n",
      "[[0.193 0.223 0.194 0.21  0.18 ]\n",
      " [0.2   0.219 0.194 0.194 0.193]\n",
      " [0.2   0.216 0.191 0.204 0.189]]\n",
      "The sums for each row (data sample) are [1. 1. 1.].\n",
      "  You should know what should be :)\n"
     ]
    }
   ],
   "source": [
    "_,_,_,probs,_ = net.forward(test_x, test_y)\n",
    "\n",
    "correct_probs = np.array([[0.193, 0.223, 0.194, 0.21 , 0.18 ],\n",
    "       [0.2  , 0.219, 0.194, 0.194, 0.193],\n",
    "       [0.2  , 0.216, 0.191, 0.204, 0.189]])\n",
    "\n",
    "print(f'Your z activation (class probabilities) is\\n{probs}')\n",
    "print(f'The correct z activation (class probabilities) is\\n{correct_probs}')\n",
    "print(f'The sums for each row (data sample) are {np.sum(probs, axis=1)}.')\n",
    "print(f'  You should know what should be :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `forward` method, focusing on loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your average loss is\n",
      "1.6301413611153333\n",
      "The correct average loss is approx\n",
      "1.6301413611153333\n"
     ]
    }
   ],
   "source": [
    "y_in, y_act ,z_in, z_act, loss = net.forward(test_x, test_y)\n",
    "correct_loss = 1.6301413611153333\n",
    "\n",
    "print(f'Your average loss is\\n{loss}')\n",
    "print(f'The correct average loss is approx\\n{correct_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `forward` method, focusing on regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your regularized average loss is\n",
      "6.321940739049807\n",
      "The correct regularized average loss is approx\n",
      "6.321940739049808\n"
     ]
    }
   ],
   "source": [
    "y_in, y_act ,z_in, z_act, loss = net.forward(test_x, test_y, reg=1000)\n",
    "correct_loss = 6.321940739049808\n",
    "\n",
    "print(f'Your regularized average loss is\\n{loss}')\n",
    "print(f'The correct regularized average loss is approx\\n{correct_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `backward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_in, y_act ,z_in, z_act, loss = net.forward(test_x, test_y, reg=0.5)\n",
    "grads = net.backward(test_x, test_y, y_in, y_act ,z_in, z_act, reg=0.5)\n",
    "\n",
    "print('Your gradient for y_wts is\\n', grads[0])\n",
    "print('Your gradient for y_b is\\n', grads[1])\n",
    "print('Your gradient for z_wts is\\n', grads[2])\n",
    "print('Your gradient for z_b is\\n', grads[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct gradients are:\n",
    "\n",
    "```\n",
    "Your gradient for y_wts is\n",
    " [[ 0.45   0.317  0.099  0.089 -0.204 -0.005  0.005]\n",
    " [-0.094 -0.001 -0.032  0.157  0.041  0.004  0.001]\n",
    " [-0.089  0.059 -0.038  0.301  0.025 -0.004 -0.013]\n",
    " [ 0.16   0.222  0.006  0.317 -0.099  0.    -0.001]\n",
    " [-0.005  0.215 -0.048  0.558 -0.037 -0.01  -0.002]\n",
    " [ 0.804  0.215  0.256 -0.807 -0.328 -0.005 -0.007]]\n",
    "Your gradient for y_b is\n",
    " [ 0.005  0.003  0.001 -0.    -0.002  0.     0.   ]\n",
    "Your gradient for z_wts is\n",
    " [[-2.977  1.034  0.1    0.971  0.858]\n",
    " [-1.734  0.724 -0.276  0.663  0.606]\n",
    " [-0.384  0.315 -0.456  0.271  0.265]\n",
    " [-3.321  1.145  0.161  1.063  0.952]\n",
    " [ 0.02   0.026 -0.097  0.019  0.022]\n",
    " [-0.001 -0.003 -0.006 -0.007 -0.001]\n",
    " [-0.001  0.011 -0.012  0.001  0.002]]\n",
    "Your gradient for z_b is\n",
    " [-0.469  0.219 -0.14   0.202  0.187]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test loss over epoch (1 of 2). \n",
    "\n",
    "The below code should generate a curve that rapidly drops to 0 (there might be fluctuations and it might not be monotonic and that's ok)\n",
    "\n",
    "Your `fit` function should show you print-outs showing:\n",
    "- Loss and validation accuracy 4 times throughout training.\n",
    "- 100% accuracy on validation set after around 5 epochs of training.\n",
    "- You are training on 20 epochs.\n",
    "- There are 20 iterations.\n",
    "- There is 1 iteration per epoch.\n",
    "\n",
    "Here is an example print-out from `fit`:\n",
    "\n",
    "    Starting to train network...There will be 20 epochs and 20 iterations total, 1 iter/epoch.\n",
    "    Completed Epoch 0/19. Training loss: 13.44. Training acc:  33.33%. Validation acc:  33.33%.\n",
    "    Completed Epoch 5/19. Training loss: 0.62. Training acc: 100.00%. Validation acc: 100.00%.\n",
    "    Completed Epoch 10/19. Training loss: 0.16. Training acc: 100.00%. Validation acc: 100.00%.\n",
    "    Completed Epoch 15/19. Training loss: 0.07. Training acc: 100.00%. Validation acc: 100.00%.\n",
    "    Finished training!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(num_features, num_hidden_units, num_classes)\n",
    "loss_hist, acc_train, acc_valid = net.fit(test_x, test_y, test_x, test_y, reg=0, print_every=5, lr=0.001, mini_batch_sz=3, n_epochs=20)\n",
    "\n",
    "print('\\nLengths of each output list:')\n",
    "print(f'{len(loss_hist)=}, {len(acc_train)=}, {len(acc_valid)=}')\n",
    "print('Each should be 20.')\n",
    "\n",
    "plot_cross_entropy_loss(loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test loss over epoch (2 of 2). \n",
    "\n",
    "The below curve should look similar.\n",
    "\n",
    "Your `fit` function should print out:\n",
    "- Loss and validation accuracy 5 times throughout training.\n",
    "- 100% accuracy on validation set after around 4 epochs of training.\n",
    "- You are training on 10 epochs.\n",
    "- There are 30 iterations.\n",
    "- There are 3 iterations per epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(num_features, num_hidden_units, num_classes)\n",
    "loss_hist, acc_train, acc_valid = net.fit(test_x, test_y, test_x, test_y, reg=0, print_every=2, lr=0.001, mini_batch_sz=1, n_epochs=10)\n",
    "\n",
    "print('\\nLengths of each output list:')\n",
    "print(f'{len(loss_hist)=}, {len(acc_train)=}, {len(acc_valid)=}')\n",
    "print('The lengths should be 30, 10, 10.')\n",
    "\n",
    "plot_cross_entropy_loss(loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Test MLP with Circle in Square dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you run your MLP on the STL-10 dataset, test it out on the simpler CIS dataset.\n",
    "\n",
    "In cells below:\n",
    "- Train an MLP using the CIS training and validation sets. Configure the MLP with the following non-default hyperparameters:\n",
    "    - 50 hidden units\n",
    "    - Learning rate of 0.5\n",
    "    - Mini-batch size of 80\n",
    "    - 600 epochs\n",
    "- Plot the loss over training iterations. You should see:\n",
    "    - A nice drop and plateau in mini-batch training loss (*with a few potentially large fluctuations along the way*).\n",
    "    - Accuracy on the validation set reach ~95%.\n",
    "- Create a scatter plot of the MLP predictions on the CIS test set. Color-code each sample by its class. Make sure your axis aspect ratios are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**: How do you interpret the circle-in-square scatterplot? Is the MLP doing a good job? \n",
    "\n",
    "**Question 6**: Play with\n",
    "- number of hidden units\n",
    "- number of epochs\n",
    "- batch size\n",
    "\n",
    "How does each parameter affect the results?\n",
    "\n",
    "**Question 7**: Do you think the single-layer net (with softmax) can handle the CIS dataset? Why or why not? (You're invited to try it, maybe as an extension :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 5**:\n",
    "\n",
    "**Answer 6**:\n",
    "\n",
    "**Answer 7**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Test on STL-10 dataset, plot performance\n",
    "\n",
    "Train an MLP on the STL-10 training set with the following non-default hyperparameters:\n",
    "- 50 hidden units\n",
    "- Learning rate of 0.1\n",
    "- Regularization strength of 0.001\n",
    "- Mini-batch size of 500\n",
    "- 100 epochs\n",
    "    \n",
    "Make two plots:\n",
    "- Plot the training loss (like usual). *Remember: the units are mini-batches.*\n",
    "- Plot the training and validation set accuracies (2 curves in one plot  include a legend, title, axis labels, etc.). *Remember: the units are epochs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**: What do the above loss and training and validation accuracy curves suggest about the quality of the hyperparameters used during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 8:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e. Optimize on STL-10 dataset with random search\n",
    "\n",
    "To optimize your MLP hyperparameters on STL-10, try a **random search** rather than a grid search. This means that instead of defining preset *values* that each hyperparameter takes on, define *ranges* (min and max values).\n",
    "\n",
    "Run your search for some $T$ iterations. On each iteration, randomly assign values to each hyperparameter within their valid ranges.\n",
    "\n",
    "Just like grid search, print out the accuracy and parameter values every time a bout of training yields the **best accuracy** on the **STL-10 validation set**. That way, if you need to stop the search prematurely, you know the current best parameter combination.\n",
    "\n",
    "Consider combinations of the following hyperparameters:\n",
    "- learning rate\n",
    "- regularization strength\n",
    "- number of hidden units\n",
    "- mini-batch size\n",
    "- number of epochs (try between `1` and `150`). *Reduce the upper limit if your machine is taking too long to train a net with this many epochs.*\n",
    "\n",
    "**Important note:** Like usual, I am not grading based on your performance numbers. I want to see that you successfully implemented the random search to find progressively better hyperparameters on STL-10.\n",
    "\n",
    "**Tips:**\n",
    "- Just like with grid search, if you find a cluster of parameters that seems promising, you can revise your search to hone in on that smaller range.\n",
    "- Turn off print outs from `fit` (adjust `verbose` argument) and only print out things related to your search.\n",
    "- If you are getting `RuntimeWarning`, your learning rate may be too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3f. Plot STL-10 results with best hyperparameters\n",
    "\n",
    "Train an MLP with the best hyperparameters that you found from your parameter search and create two plots:\n",
    "- Training STL-10 loss curve\n",
    "- Training and validation set STL-10 accuracy curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9**: Use your best trained network to compute the accuracy on the test set. What accuracy do you get?\n",
    "\n",
    "**Question 10:** Why would you use random search over grid search when optimizing parameters on a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 9**:\n",
    "\n",
    "**Answer 10:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3g. Visualize learned weights\n",
    "\n",
    "Run the `plot_weights` function to generate a grid visualization of them.\n",
    "\n",
    "You should see structure in the weights if your network is performing well. If you have a large number of hidden units, some may not be \"used\" so a subset of the weights may resemble \"noise\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_y_wts = bestNet.get_y_wts()\n",
    "best_y_wts = best_y_wts.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(wts, maxRows=25, verbose=0):\n",
    "    # limit height of figure by number of neurons\n",
    "    grid_sz = int(maxRows)\n",
    "    grid_sz = np.minimum(grid_sz, int(np.sqrt(len(wts))))\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f'Showing {grid_sz} rows')\n",
    "    \n",
    "    plt.figure(figsize=(20,20))\n",
    "    for x in range(grid_sz):\n",
    "        for y in range(grid_sz):\n",
    "            lin_ind = np.ravel_multi_index((x, y), dims=(grid_sz, grid_sz))\n",
    "            plt.subplot(grid_sz, grid_sz, lin_ind+1)\n",
    "            currImg = wts[lin_ind]\n",
    "            low, high = np.min(currImg), np.max(currImg)\n",
    "            currImg = 255*(currImg - low) / (high - low)\n",
    "            currImg = currImg.astype('uint8')\n",
    "            plt.imshow(currImg)\n",
    "            plt.gca().axis('off')\n",
    "    plt.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights(best_y_wts, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Hidden layer activation functions\n",
    "\n",
    "In this task, you will explore the effects of using different activation functions in the **hidden** layer on training. Specifically, you compare ReLU (that you already have implemented) with:\n",
    "- Sigmoid\n",
    "- Exponential linear unit (ELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp import MLP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Implement `MLP2` class and add support for several new hidden layer activation functions\n",
    "\n",
    "Implement the following methods in `MLP2` at the bottom of `mlp.py`:\n",
    "- constructor: one line to be added.\n",
    "- `forward`: Copy-and-paste from `MLP` `forward` method and modify your code to support either ReLU, sigmoid, or Leaky ReLU hidden layer activation functions.\n",
    "- `backward`: Copy-and-paste from `MLP` `forward` method and modify your code to support either ReLU, sigmoid, or Leaky ReLU hidden layer activation functions.\n",
    "\n",
    "**Note:**\n",
    "- A network should only use ONE of the hidden activation function options. Once it is set in the `MLP2` constructor, it is assumed to remain fixed forever.\n",
    "- You should only add support for different **hidden layer activation functions**  not output layer activation functions (keep that as softmax)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of hidden activation functions to implement\n",
    "\n",
    "##### 1. ReLU\n",
    "You already have this implemented.\n",
    "\n",
    "##### 2. Sigmoid\n",
    "\n",
    "Activation function:\n",
    "$$f(netIn_i) = 1 / \\left (1 + e^{-netIn_i} \\right )$$\n",
    "\n",
    "Gradient:\n",
    "$$\\frac{\\partial f}{\\partial netIn_i} = f(netIn_i) \\left (1 - f(netIn_i) \\right )$$\n",
    "\n",
    "##### 3. Exponential linear unit (ELU)\n",
    "\n",
    "Activation function:\n",
    "$$f(netIn_i) = \\begin{cases}\n",
    "  netIn_i  & netIn_i \\geq 0 \\\\\n",
    "  e^{netIn_i} - 1 & netIn_i < 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Gradient:\n",
    "$$\\frac{\\partial f}{\\partial netIn_i} = \\begin{cases}\n",
    "  1  & netIn_i \\geq 0 \\\\\n",
    "  e^{netIn_i} & netIn_i < 0\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy values for debugging\n",
    "num_inputs = 4\n",
    "num_features = 5\n",
    "num_hidden_units = 6\n",
    "num_classes = 3\n",
    "\n",
    "# Generate random data and classes\n",
    "np.random.seed(0)\n",
    "dev_x = np.random.normal(loc=0, scale=1, size=(num_inputs, num_features))\n",
    "dev_y = np.random.uniform(low=0, high=num_classes-1, size=(num_inputs,))\n",
    "dev_y = dev_y.astype(int)\n",
    "print(f'Dev input shape: {dev_x.shape}')\n",
    "print(f'Dev class vector shape: {dev_y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test (1/3): Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid\n",
    "sig_net = MLP2(num_features, num_hidden_units, num_classes, hidden_act_fun='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_in, y_act ,z_in, z_act, loss = sig_net.forward(dev_x, dev_y)\n",
    "correct_loss = 1.102608030800576\n",
    "\n",
    "print(f'Your average loss (sigmoid hidden) is\\n{loss}')\n",
    "print(f'The correct average loss is approx\\n{correct_loss}')\n",
    "print(f'The first sample hidden act values are:\\n{y_act[0]}\\nand should be\\n[0.747 0.39  0.4   0.61  0.74  0.436]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_in, y_act ,z_in, z_act, loss = sig_net.forward(dev_x, dev_y)\n",
    "grads = sig_net.backward(dev_x, dev_y, y_in, y_act ,z_in, z_act)\n",
    "\n",
    "print(f'Your gradient for y_wts (sigmoid hidden) is\\n{grads[0]}\\nand should be:')\n",
    "correct_grad = '''[[-0.001 -0.031 -0.017  0.019 -0.007  0.009]\n",
    " [-0.017  0.004 -0.001  0.017  0.013  0.011]\n",
    " [-0.004 -0.002 -0.002  0.007  0.003  0.004]\n",
    " [-0.006 -0.027 -0.016  0.023 -0.002  0.012]\n",
    " [-0.006 -0.002 -0.002  0.009  0.005  0.006]]'''\n",
    "print(correct_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test (2/3): ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elu\n",
    "leaky_net = MLP2(num_features, num_hidden_units, num_classes, hidden_act_fun='elu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_in, y_act ,z_in, z_act, loss = leaky_net.forward(dev_x, dev_y)\n",
    "correct_loss = 1.154396524333861\n",
    "\n",
    "print(f'Your average loss (elu hidden) is:\\n{loss}')\n",
    "print(f'The correct average loss is approx\\n{correct_loss}')\n",
    "print(f'The first sample hidden act values are:\\n{y_act[0]}\\nand should be\\n[1.08  0.639 0.667 0.449 1.048 0.774]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_in, y_act ,z_in, z_act, loss = leaky_net.forward(dev_x, dev_y)\n",
    "grads = leaky_net.backward(dev_x, dev_y, y_in, y_act ,z_in, z_act)\n",
    "\n",
    "print(f'Your gradient for y_wts (elu hidden) is\\n{grads[0]}\\nand should be:')\n",
    "correct_grad = '''[[-0.005 -0.094 -0.05   0.074 -0.029  0.026]\n",
    " [-0.067  0.016  0.007  0.059  0.051  0.033]\n",
    " [-0.019  0.008  0.002  0.022  0.012  0.01 ]\n",
    " [-0.028 -0.074 -0.042  0.089 -0.01   0.035]\n",
    " [-0.028  0.022  0.006  0.031  0.018  0.015]]'''\n",
    "print(correct_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test (3/3): ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu\n",
    "relu_net = MLP2(num_features, num_hidden_units, num_classes, hidden_act_fun='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_in, y_act ,z_in, z_act, loss = relu_net.forward(dev_x, dev_y)\n",
    "correct_loss = 1.0614732756912126\n",
    "\n",
    "print(f'Your average loss (relu hidden) is:\\n{loss}')\n",
    "print(f'The correct average loss is approx\\n{correct_loss}')\n",
    "print(f'The first sample hidden act values are:\\n{y_act[0]}\\nand should be\\n[1.08  0.    0.    0.449 1.048 0.   ]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_in, y_act ,z_in, z_act, loss = relu_net.forward(dev_x, dev_y)\n",
    "grads = relu_net.backward(dev_x, dev_y, y_in, y_act ,z_in, z_act)\n",
    "\n",
    "print(f'Your gradient for y_wts (relu hidden) is\\n{grads[0]}\\nand should be:')\n",
    "correct_grad = '''[[-0.006 -0.016  0.     0.077 -0.008 -0.003]\n",
    " [-0.066 -0.07   0.     0.068  0.024  0.009]\n",
    " [-0.02   0.01   0.     0.028  0.014  0.002]\n",
    " [-0.027 -0.015  0.     0.094 -0.011  0.   ]\n",
    " [-0.027  0.04   0.     0.037  0.005  0.003]]'''\n",
    "print(correct_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Explore training with each of the 3 hidden activation functions\n",
    "\n",
    "In the cell below, train 3 separate nets on STL-10 that have the exact same structure and hyperparameters, but differ in their hidden activation function (either `'sigmoid'`, `'leaky_relu'`, or `'relu'`).\n",
    "\n",
    "- Use the best MLP hyperparameters that you found in your random search above (Task 3e). \n",
    "- Train each network for `50`` epochs.\n",
    "\n",
    "Create two plots:\n",
    "1. Training loss for each of the 3 networks over training mini-batches (*you have 3 different colored curves plotted*).\n",
    "2. Validation accuracy for each of the 3 networks over training epochs (*you have 3 different colored curves plotted*).\n",
    "\n",
    "Make each plot \"high quality\" (title, axes labels, legend, good color choices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Questions\n",
    "\n",
    "**Question 11:** Rank each of the hidden layer activation functions from \"best\" to \"worst\". Explain what evidence you used to come up with the rankings.\n",
    "\n",
    "**Question 12:** Would you use any of the hidden layer activation functions instead of ReLU when training an MLP on the STL-10 dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 11:**\n",
    "\n",
    "**Answer 12:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "**Reminder**: Please do not integrate extensions into your base project so that it changes the expected behavior of core functions. It is better to duplicate the base project and add features from there.\n",
    "\n",
    "1) There are many other activation functions that are used for the hidden layer (e.g. Leaky ReLU, SELU, GELU, Softplus, Mish, etc.). Research one or more, implement them, then like in Task 4 compare training with ReLU.\n",
    "\n",
    "2) When training the networks with different hidden activation functions, the hyperparameters were determined based on the random search with the MLP that has ReLU. The hyperparameters therefore are probably is not optimal. Run hyperparameter searches with the nets configured with the different hidden activation functions and explore the extent to which performance improves.\n",
    "\n",
    "3) The `0.01` negative activation constant (`c`) in the Leaky ReLU is actually a hyperparameter and can be set to any positive value. Experiment how different `c` values influence training.\n",
    "\n",
    "4) Analyze the differences between training when sampling with replacement (i.e. not every input sample is usually processed on an epoch) and sampling without replacement (e.g. time, accuracy, loss, etc).\n",
    "\n",
    "5) Investigate how the single layer softmax network does with the CIS dataset. Explain and provide plots showing your results.\n",
    "\n",
    "6) If you have time to spare (or want to throw more computing power at the STL-10 dataset), process through the Softmax network or MLP and tune hyperparameters with the dataset at its original resolution (96x96 images). Show images of your learned weights. Can you find a training sweet spot where the learned weight visualizations look particularly cool?\n",
    "\n",
    "7) Implement a multi-class sigmoid classifier (sigmoid activation function in the output layer). I suggest creating another subclass of `SoftmaxLayer` and/or `MLP`. Compare and contrast results achieved by the softmax network.\n",
    "\n",
    "8) Explore alternative MLP architectures and compare/contrast results and performance with the ones used in the base project. For example, add one or more additional hidden layers.\n",
    "\n",
    "9)  Explore the effects of batch gradient descent, stochastic gradient descent, and mini-batch gradient descent. Make plots and interpret your results.\n",
    "\n",
    "10) Obtain, preprocess, train, and evaluate the performance of `SoftmaxLayer` and/or `MLP` on another dataset with comparable types of image features. MNIST and Fashion MNIST are good ones.\n",
    "\n",
    "11) Make a fancy coarse-to-fine grid search that automatically \"zooms in\" on the best hyperparameter combination ranges several times.\n",
    "\n",
    "12) Because each grid search training session is independent of the others, they can be parallelized. Research parallel computing / multithreading in Python to implement a parallel version of grid search or random search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
