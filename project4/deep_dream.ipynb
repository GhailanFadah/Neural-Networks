{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAMES HERE**\n",
    "\n",
    "Fall 2023\n",
    "\n",
    "CS 343: Neural Networks\n",
    "\n",
    "Project 4: Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 10:08:16.923641: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "import tf_util\n",
    "from deep_dream import DeepDream\n",
    "\n",
    "plt.show()\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: DeepDream\n",
    "\n",
    "You will make use of a pre-trained neural network (`VGG19`) to implement the **DeepDream (gradient ascent)** algorithm using TensorFlow to generate art with a neural network! The algorithm projects the receptive fields of specific filters and/or layers onto the input image to create some trippy effects! A neat side effect of this process is that you can visualize how learned weights at different levels of the network interact with parts of the input image.\n",
    "\n",
    "This task will expose you to TensorFlow's low level API that operates on image data in the Tensor data structure.\n",
    "\n",
    "### Overview\n",
    "\n",
    "1. Load in the VGG19 pre-trained network without the output layer.\n",
    "2. Make a readout model — a `Keras Model` object that allows us extract activations (`netActs`) within the subset of VGG19 network layers of our choice. Neurons in these layers of the VGG19 network will \"dream\": modify the input image to amplify patterns that the cells \"want to see\" based on their learned weight patterns. \n",
    "3. Select the types of layers (e.g. conv, pooling, etc) that we want to use to run Deep Dream on to influence the input (generated) image.\n",
    "4. Pass image through the readout network model, compute the netAct values and gradient at the selected layers.\n",
    "5. Do gradient ascent where we add a proportion of the gradient from a network layer back into the generated image for some number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Import and plot the image of Miller\n",
    "\n",
    "`miller3_224x224.jpg` is the initial image that will serve as your **generated image**.\n",
    "\n",
    "In the cell below:\n",
    "- Load in the Miller test image `miller3_224x224.jpg` at 224 x 224 resolution. This matches the resolution of images on which VGG19 was trained.\n",
    "- Normalize the image so that pixel values may span (0, 1) rather than (0, 255). Do this by dividing by 255. You should **not** use min-max normalization here (see note below)!\n",
    "- Make a plot of the image.\n",
    "\n",
    "*Why you do not want to use min-max normalization: If you have a dark image that does not have white, min-max normalization will introduce a white value when it was not there originally. This will distort the colors in your image. Dividing by the max possible pixel value (255) works around this issue.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image = Image.open(\"data/miller3_224x224.jpeg\")\n",
    "generated_img = np.asarray(raw_image)/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape is (224, 224, 3) and should be (224, 224, 3)\n",
      "Image min/max is: 0.0/1.0. It should be 0.0/1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Keep this test code\n",
    "print(f'Image shape is {generated_img.shape} and should be (224, 224, 3)')\n",
    "print(f'Image min/max is: {generated_img.min()}/{generated_img.max()}. It should be 0.0/1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Load in pre-trained VGG19 network.\n",
    "\n",
    "Load in the [pre-trained VGG19 network](https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG19) and set it to a variable called `pretrained_net`. Make sure the existing weights are not trainable and don't load/include the trained output layer.\n",
    "\n",
    "If you call the `summary()` method on the network object, you should see the following at the bottom:\n",
    "\n",
    "    Total params: 20,024,384\n",
    "    Trainable params: 0\n",
    "    Non-trainable params: 20,024,384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80134624/80134624 [==============================] - 3s 0us/step\n",
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20024384 (76.39 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 20024384 (76.39 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_net = tf.keras.applications.VGG19(include_top=False)\n",
    "pretrained_net.trainable = False\n",
    "pretrained_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Write `load_pretrained_net` to automate loading of pretrained network\n",
    "\n",
    "In `tf_util.py`, adapt your code from the previous subtask to write the `load_pretrained_net` function. This function automates the process of loading the pretrained network and paves the way for possibly loading in pre-trained networks other than VGG19.\n",
    "\n",
    "Call your function in the cell below. Calling the `summary` method on the returned pretrained network object should produce the same output as in Task 3b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20024384 (76.39 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 20024384 (76.39 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_net = tf_util.load_pretrained_net(\"vgg19\")\n",
    "pretrained_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Create a list of `VGG19` network layer names and those whose activation we want to sample\n",
    "\n",
    "In `tf_util.py`, implement the following functions:\n",
    "- `get_all_layer_strs(pretrained_net)`: Extract a list of strings from the pretrained network that represents all the layer names.\n",
    "- `filter_layer_strs(layer_names, match_str='block5')`: Filter out layer names from the complete list that have a certain substring. For example, return the subset of layers that have `'block5'` in their name. This is the list of **selected layers** whose activity we will amplify by adding their gradient back into the input image (i.e. these layers will \"do the dreaming\").\n",
    "\n",
    "Run the following test code below to check your implementations. Each function should be short (~1-5 lines of code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your full list of layers contains 22 names and it should contain 22.\n",
      "Your first layer name is input_1 and it should be input_2.\n",
      " It is not a problem if you have a different int after the underscore (e.g. input_3)\n",
      "Your last layer name is block5_pool and it should be block5_pool.\n",
      "\n",
      "Your selected layer names are:\n",
      "['block5_conv1', 'block5_conv2', 'block5_conv3', 'block5_conv4', 'block5_pool']\n",
      "and they should be\n",
      "['block5_conv1', 'block5_conv2', 'block5_conv3', 'block5_conv4', 'block5_pool']\n"
     ]
    }
   ],
   "source": [
    "# Get all layer names\n",
    "layer_names = tf_util.get_all_layer_strs(pretrained_net)\n",
    "print(f'Your full list of layers contains {len(layer_names)} names and it should contain 22.')\n",
    "print(f'Your first layer name is {layer_names[0]} and it should be input_2.')\n",
    "print(' It is not a problem if you have a different int after the underscore (e.g. input_3)')\n",
    "print(f'Your last layer name is {layer_names[-1]} and it should be block5_pool.')\n",
    "\n",
    "print()\n",
    "selected_layer_names = tf_util.filter_layer_strs(layer_names, match_str='block5')\n",
    "print(f'Your selected layer names are:\\n{selected_layer_names}\\nand they should be')\n",
    "print(\"['block5_conv1', 'block5_conv2', 'block5_conv3', 'block5_conv4', 'block5_pool']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e. Preprocess initial generated image\n",
    "\n",
    "In `tf_util.py`, implement the function `preprocess_image2tf(img, as_var)`. This function takes the initial generated image (e.g. `miller3.jpg`) in Numpy ndarray format and converts it to a TensorFlow Variable tensor.\n",
    "\n",
    "In the cell below, call your function to preprocess the test image of Miller as a TensorFlow Variable. Name the preprocessed image `generated_img_tf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_img_tf = tf_util.preprocess_image2tf(np.asarray(Image.open(\"data/miller3_224x224.jpeg\")), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the initial generated image is (1, 224, 224, 3) and should be (1, 224, 224, 3)\n",
      "The min/max of the initial generated image is 0.0/1.0 and should be 0.0/1.0\n",
      "The datatype of the initial generated image is <dtype: 'float32'> and should be <dtype: 'float32'>\n",
      "The initial generated image is trainable? False. It should be!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Keep below test code\n",
    "print(f'The shape of the initial generated image is {generated_img_tf.shape} and should be (1, 224, 224, 3)')\n",
    "print(f'The min/max of the initial generated image is {tf.reduce_min(generated_img_tf)}/{tf.reduce_max(generated_img_tf)} and should be 0.0/1.0')\n",
    "print(f\"The datatype of the initial generated image is {generated_img_tf.dtype} and should be <dtype: 'float32'>\")\n",
    "print(f'The initial generated image is trainable? {hasattr(generated_img_tf, \"trainable\")}. It should be!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3f. Build VGG19 selected layer readout model\n",
    "\n",
    "When we do the forward pass with the generated image (e.g. `miller3`), we get activations in every network layer. However, we only want a select group of layers to influence the input image (e.g. only those with `block5` in the name). We could in principle take the list of ALL the netActs and extract the netActs we want according to indices of the desired layers, but there is more overhead with this approach since there are a lot of netAct values we won't use! It is more convenient to build a `tf.keras.Model` object based on the pretrained network that returns a list of netActs *in only the layers we specify*. We will call this model that returns the subset of layer netActs that we care about **the readout model**.\n",
    "\n",
    "In `tf_util.py`, implement the function `make_readout_model(pretrained_net, layer_names)` that builds the readout model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers of netAct returned is 5 and should be 5\n",
      "Shape of first layer of netActs is (1, 14, 14, 512) and should be (1, 14, 14, 512)\n",
      "First few netAct values of first layer filters in top-left corner:\n",
      "[1.415 0.    4.176 0.    0.   ]\n",
      "and they should be:\n",
      "[1.415 0.    4.176 0.    0.   ]\n",
      "Shape of last layer of netActs is (1, 7, 7, 512) and should be (1, 7, 7, 512)\n",
      "First few netAct values of last layer filters in top-left corner:\n",
      "[0.394 0.    0.    0.    0.554]\n",
      "and they should be:\n",
      "[0.394 0.    0.    0.    0.554]\n"
     ]
    }
   ],
   "source": [
    "test_readout_model = tf_util.make_readout_model(pretrained_net, selected_layer_names)\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "test_input = tf.random.uniform(shape=(1, 224, 224, 3))\n",
    "test_net_acts = test_readout_model(test_input)\n",
    "print(f'Number of layers of netAct returned is {len(test_net_acts)} and should be 5')\n",
    "print(f'Shape of first layer of netActs is {test_net_acts[0].shape} and should be (1, 14, 14, 512)')\n",
    "print(f'First few netAct values of first layer filters in top-left corner:\\n{test_net_acts[0][0,0,0,:5]}')\n",
    "print('and they should be:')\n",
    "print('[1.415 0.    4.176 0.    0.   ]')\n",
    "print(f'Shape of last layer of netActs is {test_net_acts[-1].shape} and should be (1, 7, 7, 512)')\n",
    "print(f'First few netAct values of last layer filters in top-left corner:\\n{test_net_acts[-1][0,0,0,:5]}')\n",
    "print('and they should be:')\n",
    "print('[0.394 0.    0.    0.    0.554]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3g. Start implementing Deep Dream network\n",
    "\n",
    "Implement the following `DeepDream` class methods in `deep_dream.py`:\n",
    "- Constructor\n",
    "- `loss_layer(self, layer_net_acts)`: Computes one selected layer's contribution to the total loss. This is the mean activation (`netAct`) across the layer.\n",
    "- `forward(self, gen_img, eps=1e-8, standardize_grads=True)`: Does the forward pass with the generated image, computes the total loss, and computes the gradient of the loss with respect to the input generated image. If `standardize_grads` is set to `True`, the image gradients should be standardized according to the usual formula (see below).\n",
    "\n",
    "#### Image gradient standardization\n",
    "\n",
    "The following equation computes the standardized image gradient $\\hat{g}_{i,j}$ at pixel $(i,j)$ in the image gradient.\n",
    "\n",
    "$$\n",
    "\\hat{g}_{i,j} = \\frac{g_{i,j} - \\mu_g}{\\sigma_g + \\epsilon}\n",
    "$$\n",
    "\n",
    "where $g_{i,j}$ is the original/raw gradient of the image $(i,j)$ with respect to the total loss (i.e. `d_image`), $\\mu_g$ is the mean of the image gradients, $\\sigma_g$ is the standard deviation of the image gradients, and $\\epsilon$ a small numer (e.g. `1e-8`) to prevent possible division by 0.\n",
    "\n",
    "Notes:\n",
    "- Remember that the shape of `d_thing` has the same shape as `thing`. So `d_image` has shape `(224, 224, 3)` — every pixel in the original image gets a gradient update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Test `loss_layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dd = DeepDream(pretrained_net, selected_layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your test layer loss is 0.49835 and should be 0.49835\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "test_layer_net_acts = tf.random.uniform(shape=(1, 14, 14, 512))\n",
    "test_layer_loss = test_dd.loss_layer(test_layer_net_acts)\n",
    "print(f'Your test layer loss is {test_layer_loss:.5f} and should be 0.49835')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Test `forward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your total loss is 0.26632 and should be 0.26632\n",
      "The shape of your image gradients are (1, 224, 224, 3) and it should be (1, 224, 224, 3)\n",
      "The first few gradients are\n",
      "[-0.137  0.375  0.997 -0.307 -1.642]\n",
      "and they should be\n",
      "[-0.137  0.375  0.997 -0.307 -1.642]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "test_img_tf = tf.Variable(tf.random.uniform(shape=(1, 224, 224, 3), minval=0, maxval=1))\n",
    "test_loss, test_grads = test_dd.forward(test_img_tf)\n",
    "print(f'Your total loss is {test_loss:.5f} and should be 0.26632')\n",
    "print(f'The shape of your image gradients are {test_grads.shape} and it should be (1, 224, 224, 3)')\n",
    "print(f'The first few gradients are\\n{test_grads[0,0,:5,0]}\\nand they should be\\n[-0.137  0.375  0.997 -0.307 -1.642]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3h. Implement `fit`: use gradient ascent algorithm to iteratively modify generated image\n",
    "\n",
    "The `fit` method uses **gradient ascent** to update the generated image on every epoch — i.e. the network modifies the input image across epochs based on the image gradients. The result is a surreal, trippy, dreamy effect that merges the neural and image representations.\n",
    "\n",
    "#### Gradient ascent\n",
    "\n",
    "$$\n",
    "I(t+1) = I(t) + \\alpha \\times \\hat{g}\n",
    "$$\n",
    "\n",
    "where $I$ is the generated image on epoch $t$, $\\alpha$ is the learning rate, and $\\hat{g}$ is the image gradient on the current epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Deep Dream for 1 epoch\n",
    "\n",
    "The code below runs `fit` for one epoch.\n",
    "- You should print out the estimated amount of time in minutes to complete `E` epochs.\n",
    "- You should print out how many epochs have been completed out of `E` total. This should happen on the first epoch (and in general `print_every` epochs).\n",
    "- `fit` should generate a plot showing the generated image after 1 epoch. *You should see cool shapes starting to emerge in the generated image*! To help make this happen, implement `tf2image(tensor)` in `tf_util.py` that converts the TensorFlow tensor into a PIL `Image` object.\n",
    "- You should export a JPG of the generated image in the `deep_dream_output` folder within your project working directory (*create it if it doesn't exist*). \n",
    "\n",
    "Here is an example progress printout with the estimated time:\n",
    "\n",
    "```\n",
    "Epoch 0/0 completed\n",
    "  1 epoch took 0.010 mins. Expected runtime is 0.010 mins.\n",
    "```\n",
    "\n",
    "*Note: the `tf.identity` makes and returns a copy of `generated_img_tf` — this way we do not modify the original Tensor of the image during this test.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 224, 3), |u1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/PIL/Image.py:2992\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2991\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2992\u001b[0m     mode, rawmode \u001b[39m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[1;32m   2993\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 224, 3), '|u1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ghailanfadah/Desktop/CH343/project4/deep_dream.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ghailanfadah/Desktop/CH343/project4/deep_dream.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m gen_img_1epoch \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mVariable(tf\u001b[39m.\u001b[39midentity(generated_img_tf))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ghailanfadah/Desktop/CH343/project4/deep_dream.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m loss_hist \u001b[39m=\u001b[39m test_dd\u001b[39m.\u001b[39;49mfit(gen_img_1epoch, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ghailanfadah/Desktop/CH343/project4/deep_dream.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTest code:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ghailanfadah/Desktop/CH343/project4/deep_dream.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mYour loss over 1 epoch is \u001b[39m\u001b[39m{\u001b[39;00mloss_hist[\u001b[39m0\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m and should be 1.7833\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/CH343/project4/deep_dream.py:146\u001b[0m, in \u001b[0;36mDeepDream.fit\u001b[0;34m(self, gen_img, n_epochs, lr, print_every, plot, plot_fig_sz, export)\u001b[0m\n\u001b[1;32m    144\u001b[0m gen_img\u001b[39m.\u001b[39massign(tf\u001b[39m.\u001b[39mclip_by_value(gen_img, clip_value_min\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, clip_value_max\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m plot:\n\u001b[0;32m--> 146\u001b[0m     image \u001b[39m=\u001b[39m tf_util\u001b[39m.\u001b[39;49mtf2image(gen_img)\n\u001b[1;32m    147\u001b[0m     fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39mplot_fig_sz)\n\u001b[1;32m    148\u001b[0m     plt\u001b[39m.\u001b[39mimshow(image)\n",
      "File \u001b[0;32m~/Desktop/CH343/project4/tf_util.py:153\u001b[0m, in \u001b[0;36mtf2image\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    150\u001b[0m tf\u001b[39m.\u001b[39msqueeze(scale_ten)\n\u001b[1;32m    151\u001b[0m array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(scale_ten)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n\u001b[0;32m--> 153\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mfromarray(array)\n\u001b[1;32m    155\u001b[0m \u001b[39mreturn\u001b[39;00m img\n\u001b[1;32m    156\u001b[0m \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/PIL/Image.py:2994\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2992\u001b[0m         mode, rawmode \u001b[39m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[1;32m   2993\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 2994\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot handle this data type: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m typekey) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   2995\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2996\u001b[0m     rawmode \u001b[39m=\u001b[39m mode\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 224, 3), |u1"
     ]
    }
   ],
   "source": [
    "gen_img_1epoch = tf.Variable(tf.identity(generated_img_tf))\n",
    "loss_hist = test_dd.fit(gen_img_1epoch, n_epochs=1, lr=0.1)\n",
    "print('Test code:')\n",
    "print(f'Your loss over 1 epoch is {loss_hist[0]:.4f} and should be 1.7833')\n",
    "print(f'Generated image min/max pixel values are {tf.reduce_min(gen_img_1epoch)}/{tf.reduce_max(gen_img_1epoch)} and should be 0.0/1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3i. Run Deep Dream on `miller3_224x224.jpg`\n",
    "\n",
    "In the cell below, copy-and-paste code that you've written above to:\n",
    "- Load in and preprocess the initial generated image (`miller3_224x224.jpg` here).\n",
    "- Load in the pretrained VGG19 network.\n",
    "- Creating a `DeepDream` network and fitting it to the image over `26` epochs with the default learning rate of `0.01`.\n",
    "- Only activations in layers with `block5` in the name should contribute to the loss and lead to modifications of the generated image.\n",
    "\n",
    "The goal is to run the complete Deep Dream algorithm on a generated image without having to execute all the cells above.\n",
    "\n",
    "The cell below should show `miller3` after running fit for 26 epochs. *Even if your computer is a few years old, this should take at most 5 minutes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3j. Refine Deep Dream by running it with multiple image scales\n",
    "\n",
    "While the existing Deep Dream algorithm results in some cool effects, a little additional work can make the Deep Dream images look even better.\n",
    "\n",
    "Implement `fit_multiscale` in `deep_dream.py`. In this version you call `fit` to run the gradient ascent algorithm on the generated image over multiple runs. Each time you finish a run, scale (resize) the image so that you run gradient ascent again on the larger version of image.\n",
    "\n",
    "Since this is effectively repeating `gradient_ascent` a set number of times (`n_scales`), it will take longer to finish computing.\n",
    "\n",
    "#### Test `fit_multiscale`\n",
    "\n",
    "Copy-and-paste your code above from Task 3i, but now run `fit_multiscale` instead of `fit`. Run it with default parameters for now. The cell below should:\n",
    "- show the generated image (`miller3`) after gradient ascent is run on the generated image at each scale. \n",
    "- print outs showing the progress (i.e. number of scales currently completed).\n",
    "- Estimated total runtime based on the first scale.\n",
    "\n",
    "*Even if your computer is several years old, this should take at most 5-10 mins to finish. If it is taking too long and you think that your code is working properly, run it in Davis 102 or cut down on the number of epochs and scales.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3k. Run Deep Dream on at least one other image\n",
    "\n",
    "Copy-and-paste your code again and run either the single-scale or multi-scale version of Deep Dream on another image of your choice. It could be one of the other provided images. Adjust hyperparameters as necessary so that the generated image bears some resemblance to the original image (i.e. the generated image is not totally dominated by the network influence).\n",
    "\n",
    "**Note about the size of your image:**\n",
    "- You can resize your image to 224x224, the resolution of images VGG19 was trained on.\n",
    "- If you have a more powerful machine (or are willing to wait longer for prettier pictures), you can use higher resolution images. But you should start with the 224 x 224 version for debugging purposes. You can use the PIL Image resize method. Alternatively, there are numerous external tools (e.g. ImageMagick) and other Python-based ways to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3l. Questions\n",
    "\n",
    "Generate one or more Deep Dream images to accompany each of your written answers to the following questions. You can include them inline with your answers below.\n",
    "\n",
    "**Question 7:** Give at least two reasons why the multi-scale version tends to produce better visualizations. *Hint: think about the filter receptive fields involved in the process.*\n",
    "\n",
    "**Question 8**: What differences do you notice if you only use earlier/later network layers to do the Deep Dream process?\n",
    "\n",
    "**Question 9**: Describe what you notice when you let pooling vs different conv layers contribute to the generated image.\n",
    "\n",
    "**Question 10**: In your own words, describe why a certain animal, texture, or shape appears in a particular spot in the generated image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 7:** \n",
    "\n",
    "**Answer 8**: \n",
    "\n",
    "**Answer 9**: \n",
    "\n",
    "**Answer 10**: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
